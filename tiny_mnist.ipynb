{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26db1ccf-e6a0-403e-93b3-1e76da541755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Environment & library imports\n",
    "# -----------------------------------------------------------\n",
    "import torch\n",
    "from   torch import nn, optim\n",
    "from   torchvision import datasets, transforms\n",
    "from   torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "# ---- Reproducibility (optional) ----------------------------\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True      #  use only deterministic convolution kernels (repeatable outputs, slower).\n",
    "torch.backends.cudnn.benchmark     = False     # disable algorithm auto-tuning (consistent algorithm choice, no profiling overhead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed440d9-ed1c-4fad-8651-d82a0f07651c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 60000 | Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "#   Dataset & DataLoader\n",
    "# -----------------------------------------------------------\n",
    "transform = transforms.ToTensor()          # (0-255) → (0-1) float32\n",
    "batch_sz  = 64\n",
    "\n",
    "train_ds = datasets.MNIST(root=\".\", train=True, download=True,\n",
    "                          transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\".\", train=False, download=True,\n",
    "                          transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_sz, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds , batch_size=256,    shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_ds)} | Test samples: {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c1922ef-b0eb-46db-b892-b48c1176b03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 108,330\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "#   Model definition \n",
    "# -----------------------------------------------------------\n",
    "class TinyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal CNN for MNIST:\n",
    "        Conv(1→16, 3×3) → ReLU\n",
    "        Flatten\n",
    "        Linear(16*26*26 → 10)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1,  out_channels=16,\n",
    "                              kernel_size=3, stride=1, padding=0)\n",
    "        self.fc   = nn.Linear(16 * 26 * 26, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv(x))       # shape: [B,16,26,26]\n",
    "        x = x.view(x.size(0), -1)          # flatten  → [B,16*26*26]\n",
    "        logits = self.fc(x)                # → [B,10]\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = TinyCNN()\n",
    "print(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c89264bc-8a00-43f4-a1dd-b807f40f80d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | loss 0.390 | test acc 91.90%\n",
      "Epoch 2 | loss 0.283 | test acc 92.34%\n",
      "Epoch 3 | loss 0.228 | test acc 94.73%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "#   Training loop\n",
    "# -----------------------------------------------------------\n",
    "epochs  = 3                        # hits ~95+ % in < 30 s on CPU\n",
    "lr      = 0.01\n",
    "moment  = 0.7\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=moment)\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss   = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # quick validation each epoch\n",
    "    model.eval(); correct = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            preds = model(imgs).argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    acc = 100 * correct / len(test_ds)\n",
    "    print(f\"Epoch {ep} | loss {running_loss/len(train_loader):.3f} \"\n",
    "          f\"| test acc {acc:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "130fb2bd-1ed0-4627-95d2-ac2aa02cd625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ONNX file saved & structurally valid\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 4 · Export FP32 weights to ONNX\n",
    "# -----------------------------------------------------------\n",
    "import torch\n",
    "import torch.onnx   as onnx_export\n",
    "import onnx         # runtime checker\n",
    "\n",
    "model.eval()                            # inference mode\n",
    "dummy = torch.randn(1, 1, 28, 28)       # (N,C,H,W)  -- batch size 1\n",
    "\n",
    "onnx_path = \"mnist_cnn.onnx\"\n",
    "onnx_export.export(\n",
    "    model, dummy, onnx_path,\n",
    "    input_names  = ['input'],\n",
    "    output_names = ['logits'],\n",
    "    opset_version=13,                   # compatible with TVM ≥0.14\n",
    "    dynamic_axes = {\n",
    "        'input':  {0: 'batch'},         # allow N>1 at runtime\n",
    "        'logits': {0: 'batch'}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sanity-check the file\n",
    "onnx.checker.check_model(onnx_path)\n",
    "print(\"✓ ONNX file saved & structurally valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea0cebc-e7b8-44be-8cdf-1ea33c66db7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving 'mnist_cnn.onnx' at http://localhost:8080\n",
      "Serving 'mnist_cnn.onnx' at http://localhost:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600\"\n",
       "            src=\"http://localhost:10080\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fdac825bdf0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netron\n",
    "# Option A – open in a new browser tab (default port 8080)\n",
    "netron.start(\"mnist_cnn.onnx\")\n",
    "\n",
    "# Option B – embed as an <iframe> inside JupyterLab\n",
    "from IPython.display import IFrame\n",
    "netron_port = 10080\n",
    "netron.start(\"mnist_cnn.onnx\")\n",
    "IFrame(src=f\"http://localhost:{netron_port}\", width=\"100%\", height=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edbfb73d-b1e3-4382-b46c-039d89da6a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using injective.cpu for less based on highest priority (10)\n",
      "Using injective.cpu for take based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using concatenate.cpu for concatenate based on highest priority (10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#[version = \"0.0.5\"]\n",
      "def @main(%input: Tensor[(1, 1, 28, 28), float32] /* ty=Tensor[(1, 1, 28, 28), float32] span=/conv/Conv.input:0:0 */) -> Tensor[(1, 10), float32] {\n",
      "  %0 = nn.conv2d(%input, meta[relay.Constant][0] /* ty=Tensor[(16, 1, 3, 3), float32] span=/conv/Conv.conv.weight:0:0 */, padding=[0, 0, 0, 0], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 26, 26), float32] span=/conv/Conv:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] span=/conv/Conv.conv.bias:0:0 */) /* ty=Tensor[(1, 16, 26, 26), float32] span=/conv/Conv:0:0 */;\n",
      "  %2 = nn.relu(%1)  ...\n",
      "\n",
      "Relay graph imported | params: 4 tensors\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 5 · Bring the ONNX graph into TVM Relay IR\n",
    "# -----------------------------------------------------------\n",
    "import tvm\n",
    "from   tvm import relay\n",
    "\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "shape_dict = {\"input\": (1, 1, 28, 28)}  # batch=1 placeholder\n",
    "\n",
    "mod_fp32, params_fp32 = relay.frontend.from_onnx(\n",
    "    onnx_model,\n",
    "    shape =shape_dict,\n",
    "    freeze_params=False                 # weights baked into graph\n",
    ")\n",
    "\n",
    "print(mod.astext(show_meta_data=False)[:600], \"...\\n\")\n",
    "print(f\"Relay graph imported | params: {len(params_fp32)} tensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "492fe886-c980-463f-b405-5dc04237a3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for round based on highest priority (10)\n",
      "Using injective.cpu for clip based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for round based on highest priority (10)\n",
      "Using injective.cpu for clip based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for round based on highest priority (10)\n",
      "Using injective.cpu for clip based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for fixed_point_multiply based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#[version = \"0.0.5\"]\n",
      "def @main(%input: Tensor[(1, 1, 28, 28), float32] /* ty=Tensor[(1, 1, 28, 28), float32] span=/conv/Conv.input:0:0 */) -> Tensor[(1, 10), float32] {\n",
      "  %0 = multiply(%input, 16f /* ty=float32 */) /* ty=Tensor[(1, 1, 28, 28), float32] */;\n",
      "  %1 = round(%0) /* ty=Tensor[(1, 1, 28, 28), float32] */;\n",
      "  %2 = clip(%1, a_min=-127f, a_max=127f) /* ty=Tensor[(1, 1, 28, 28), float32] */;\n",
      "  %3 = cast(%2, dtype=\"int8\") /* ty=Tensor[(1, 1, 28, 28), int8] */;\n",
      "  %4 = nn.conv2d(%3, meta[relay.Constant][0] /* ty=Tensor[(16, 1, 3, 3), int8] */, padding=[0, 0, 0, 0], channels=16, kernel_size=[3, 3], out_dtype=\"int32\") /* ty=Tensor[(1, 16, 26, 26), int32] */;\n",
      "  %5 = cast(%4, dtype=\"int64\") /* ty=Tensor[(1, 16, 26, 26), int64] */;\n",
      "  %6 = fixed_point_multiply(%5, multiplier=1455275264, shift=-7) /* ty=Tensor[(1, 16, 26, 26), int64] */;\n",
      "  %7 = clip(%6, a_min=-127f, a_max=127f) /* ty=Tensor[(1, 16, 26, 26), int64] */;\n",
      "  %8 = cast(%7, dtype=\"int32\") /* ty=Tensor[(1, 16, 26, 26), int32] */;\n",
      "   ...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 6 · Fast INT8 quantisation (global scale method)\n",
    "# -----------------------------------------------------------\n",
    "from tvm import relay\n",
    "\n",
    "with relay.quantize.qconfig(\n",
    "        calibrate_mode   = 'global_scale',\n",
    "        global_scale     = 8.0,\n",
    "        weight_scale     = 'max',\n",
    "        skip_conv_layers = [],     # ← quantise every conv\n",
    "        skip_dense_layer = False   # ← quantise the FC too\n",
    "):\n",
    "    mod_int8 = relay.quantize.quantize(mod_fp32, params_fp32)\n",
    "\n",
    "# Make sure types are inferred **after** quantisation\n",
    "mod_int8 = relay.transform.InferType()(mod_int8)\n",
    "\n",
    "print(mod_int8.astext(show_meta_data=False)[:1000], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "981fd868-eb95-4b5a-811a-2afc13599f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download pre-tuned parameters package from https://raw.githubusercontent.com/tlc-pack/tophub/main/tophub/llvm_v0.04.log\n",
      "Downloading from url https://raw.githubusercontent.com/tlc-pack/tophub/main/tophub/llvm_v0.04.log to /home/andres/.tvm/tophub/llvm_v0.04.log\n",
      "Using pad.generic for nn.pad based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using reduce.cpu for sum based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n",
      "Using conv2d_nchw_int8.x86 for nn.conv2d based on highest priority (10)\n",
      "Using dense_pack.x86 for nn.dense based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for round based on highest priority (10)\n",
      "Using injective.cpu for clip based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for add based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using pad.generic for nn.pad based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using conv2d_NCHWc_int8.x86 for nn.contrib_conv2d_NCHWc based on highest priority (10)\n",
      "Using injective.cpu for subtract based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for fixed_point_multiply based on highest priority (10)\n",
      "Using injective.cpu for clip based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for add based on highest priority (10)\n",
      "Using injective.cpu for nn.relu based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using injective.cpu for reshape based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for round based on highest priority (10)\n",
      "Using injective.cpu for clip based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using dense_pack.x86 for nn.contrib_dense_pack based on highest priority (10)\n",
      "Using injective.cpu for add based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVM INT8 latency: 0.158 ms  (batch 1)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 7 · Compile INT8 graph and time it\n",
    "# -----------------------------------------------------------\n",
    "target = \"llvm -mcpu=core-avx2\"      # adjust if you’re on ARM / Apple Silicon\n",
    "dev    = tvm.cpu()\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod_int8, target=target, params=params_fp32)\n",
    "\n",
    "module = tvm.contrib.graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "\n",
    "import numpy as np, time\n",
    "data = np.random.rand(1,1,28,28).astype(\"float32\")\n",
    "module.set_input(\"input\", data)\n",
    "\n",
    "# warm-up\n",
    "for _ in range(10):\n",
    "    module.run()\n",
    "\n",
    "# timing\n",
    "t = module.module.time_evaluator(\"run\", dev, repeat=100)\n",
    "tvm_ms = np.median(t().results) * 1000\n",
    "print(f\"TVM INT8 latency: {tvm_ms:.3f} ms  (batch 1)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bea1f5da-0e56-45ec-9b8c-edb3b0810b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch FP32 latency: 0.239 ms\n",
      "Speed-up: 1.51×  (INT8 vs. FP32)\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # warm-up\n",
    "    for _ in range(10):\n",
    "        _ = model(torch.from_numpy(data))\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = model(torch.from_numpy(data))\n",
    "    dt = time.time() - start          # total time for 100 runs\n",
    "\n",
    "pt_ms = dt / 100 * 1000              # ← divide, not multiply\n",
    "print(f\"PyTorch FP32 latency: {pt_ms:.3f} ms\")\n",
    "\n",
    "print(f\"Speed-up: {pt_ms / tvm_ms:.2f}×  (INT8 vs. FP32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac63c3e-6374-44c9-aea6-a961c03bb9f1",
   "metadata": {},
   "source": [
    "## ───────────────────────────────────────────\n",
    "## Variant 1 : Conv + ReLU + MaxPool (Batch-1)\n",
    "## ───────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aeb6a122-a0a2-48ae-91a0-df8edcdae6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count: 27,210\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "#   TinyCNN-Pool: Conv → ReLU → MaxPool → Flatten → Dense\n",
    "# -----------------------------------------------------------\n",
    "class TinyCNNPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv(1→16, 3×3) → ReLU → MaxPool(2×2)\n",
    "    → Flatten → Dense(13*13*16 → 10)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 16, 3)          # out: 26×26×16\n",
    "        self.pool = nn.MaxPool2d(2, 2)           # out: 13×13×16\n",
    "        self.fc   = nn.Linear(16*13*13, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = TinyCNNPool()\n",
    "print(f\"Param count: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e2558de-3569-4cbe-a095-0ee93681b25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 94.54%\n",
      "Epoch 2: 96.89%\n",
      "Epoch 3: 97.58%\n"
     ]
    }
   ],
   "source": [
    "# same loaders as before\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "\n",
    "# training loop for 3 epochs\n",
    "for ep in range(3):\n",
    "    model.train()\n",
    "    for imgs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        criterion(model(imgs), labels).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # val\n",
    "    model.eval(); correct = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            correct += (model(imgs).argmax(1) == labels).sum().item()\n",
    "    print(f\"Epoch {ep+1}: {100*correct/len(test_ds):.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_cnn_pool.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3a53ca1-0357-42fa-9669-dd645c2dac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX\n",
    "dummy = torch.randn(1, 1, 28, 28)\n",
    "torch.onnx.export(\n",
    "    model, dummy, \"mnist_cnn_pool.onnx\",\n",
    "    input_names=['input'], output_names=['logits'],\n",
    "    opset_version=13,\n",
    "    dynamic_axes={'input':{0:'batch'}, 'logits':{0:'batch'}}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f28566e-9a4e-4586-abe6-d0e0dc12f724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using injective.cpu for less based on highest priority (10)\n",
      "Using injective.cpu for take based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using concatenate.cpu for concatenate based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for round based on highest priority (10)\n",
      "Using injective.cpu for clip based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for round based on highest priority (10)\n",
      "Using injective.cpu for clip based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for round based on highest priority (10)\n",
      "Using injective.cpu for clip based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for fixed_point_multiply based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#[version = \"0.0.5\"]\n",
      "def @main(%input: Tensor[(1, 1, 28, 28), float32] /* ty=Tensor[(1, 1, 28, 28), float32] span=/conv/Conv.input:0:0 */) -> Tensor[(1, 10), float32] {\n",
      "  %0 = multiply(%input, 16f /* ty=float32 */) /* ty=Tensor[(1, 1, 28, 28), float32] */;\n",
      "  %1 = round(%0) /* ty=Tensor[(1, 1, 28, 28), float32] */;\n",
      "  %2 = clip(%1, a_min=-127f, a_max=127f) /* ty=Tensor[(1, 1, 28, 28), float32] */;\n",
      "  %3 = cast(%2, dtype=\"int8\") /* ty=Tensor[(1, 1, 28, 28), int8] */;\n",
      "  %4 = nn.conv2d(%3, meta[relay.Constant][0] /* ty=Tensor[(16, 1, 3, 3), int8] */, padding=[0, 0, 0, 0], channels=16, kernel_size=[3, 3], out_dtype=\"int32\") /* ty=Tensor[(1, 16, 26, 26), int32] */;\n",
      "  %5 = cast(%4, dtype=\"int64\") /* ty=Tensor[(1, 16, 26, 26), int64] */;\n",
      "  %6 = fixed_point_multiply(%5, multiplier=1261564800, shift=-6) /* ty=Tensor[(1, 16, 26, 26), int64] */;\n",
      "  %7 = clip(%6, a_min=-127f, a_max=127f) /* ty=Tensor[(1, 16, 26, 26), int64] */;\n",
      "  %8 = cast(%7, dtype=\"int32\") /* ty=Tensor[(1, 16, 26, 26), int32] */;\n",
      "   ...\n"
     ]
    }
   ],
   "source": [
    "# Re-import and quantise\n",
    "\n",
    "onnx_model = onnx.load(\"mnist_cnn_pool.onnx\")\n",
    "shape = {\"input\": (1,1,28,28)}\n",
    "\n",
    "mod_fp32, params_fp32 = relay.frontend.from_onnx(\n",
    "    onnx_model, shape=shape, freeze_params=False\n",
    ")\n",
    "\n",
    "with relay.quantize.qconfig(\n",
    "        calibrate_mode='global_scale', global_scale=8.0,\n",
    "        weight_scale='max', skip_conv_layers=[], skip_dense_layer=False):\n",
    "    mod_int8 = relay.quantize.quantize(mod_fp32, params_fp32)\n",
    "\n",
    "mod_int8 = relay.transform.InferType()(mod_int8)\n",
    "print(mod_int8.astext(show_meta_data=False)[:1000], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1151a739-3cd1-4715-8031-3a0707a1afff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad.generic for nn.pad based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using reduce.cpu for sum based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using conv2d_nchw_int8.x86 for nn.conv2d based on highest priority (10)\n",
      "Using dense_pack.x86 for nn.dense based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for round based on highest priority (10)\n",
      "Using injective.cpu for clip based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for add based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using pad.generic for nn.pad based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using conv2d_NCHWc_int8.x86 for nn.contrib_conv2d_NCHWc based on highest priority (10)\n",
      "Using injective.cpu for subtract based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for fixed_point_multiply based on highest priority (10)\n",
      "Using injective.cpu for clip based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for add based on highest priority (10)\n",
      "Using injective.cpu for nn.relu based on highest priority (10)\n",
      "Using pool.cpu for nn.max_pool2d based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using injective.cpu for reshape based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n",
      "Using injective.cpu for round based on highest priority (10)\n",
      "Using injective.cpu for clip based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using dense_pack.x86 for nn.contrib_dense_pack based on highest priority (10)\n",
      "Using injective.cpu for add based on highest priority (10)\n",
      "Using injective.cpu for cast based on highest priority (10)\n",
      "Using injective.cpu for multiply based on highest priority (10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVM INT8 + MaxPool: 0.064 ms\n",
      "PyTorch FP32 + MaxPool: 0.112 ms\n",
      "Speed-up: 1.74×\n"
     ]
    }
   ],
   "source": [
    "# Build and benchmark\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod_int8, target=target, params=params_fp32)\n",
    "\n",
    "module = tvm.contrib.graph_executor.GraphModule(lib['default'](dev))\n",
    "for _ in range(10):\n",
    "    module.set_input('input', np.random.rand(1,1,28,28).astype('float32'))\n",
    "    module.run()\n",
    "ft = module.module.time_evaluator(\"run\", dev, repeat=100)\n",
    "tvm_ms = np.median(ft().results)*1000\n",
    "print(f\"TVM INT8 + MaxPool: {tvm_ms:.3f} ms\")\n",
    "\n",
    "# PyTorch baseline\n",
    "model.eval(); data = torch.randn(1,1,28,28)\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = model(data)\n",
    "pt_ms = (time.time()-start)/100*1000\n",
    "print(f\"PyTorch FP32 + MaxPool: {pt_ms:.3f} ms\")\n",
    "print(f\"Speed-up: {pt_ms/tvm_ms:.2f}×\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc4c2cf-ff56-405b-809b-5d6b6128fe3b",
   "metadata": {},
   "source": [
    "## ───────────────────────────────────────────\n",
    "## Batch-size 8 timing\n",
    "## ───────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27a70fe7-48cb-4091-ad94-a5089effa56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using injective.cpu for less based on highest priority (10)\n",
      "Using injective.cpu for take based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using concatenate.cpu for concatenate based on highest priority (10)\n",
      "Using conv2d_nchw.x86 for nn.conv2d based on highest priority (10)\n",
      "Using dense_pack.x86 for nn.dense based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using injective.cpu for expand_dims based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using conv2d_NCHWc.x86 for nn.contrib_conv2d_NCHWc based on highest priority (10)\n",
      "Using injective.cpu for add based on highest priority (10)\n",
      "Using injective.cpu for nn.relu based on highest priority (10)\n",
      "Using pool.cpu for nn.max_pool2d based on highest priority (10)\n",
      "Using layout_transform.generic for layout_transform based on highest priority (10)\n",
      "Using injective.cpu for reshape based on highest priority (10)\n",
      "Using dense_pack.x86 for nn.contrib_dense_pack based on highest priority (10)\n",
      "Using injective.cpu for add based on highest priority (10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVM INT8 batch-8 : 0.229 ms  → 0.029 ms / img\n"
     ]
    }
   ],
   "source": [
    "# 1 · Re-import ONNX with batch 8 shape\n",
    "from tvm import relay\n",
    "onnx_model = onnx.load(\"mnist_cnn_pool.onnx\")\n",
    "\n",
    "shape8 = {\"input\": (8, 1, 28, 28)}\n",
    "mod8, params8 = relay.frontend.from_onnx(\n",
    "    onnx_model, shape=shape8, freeze_params=False\n",
    ")\n",
    "\n",
    "# 2 · Quantise exactly as before\n",
    "with relay.quantize.qconfig(calibrate_mode='global_scale',\n",
    "                            global_scale=8.0,\n",
    "                            weight_scale='max'):\n",
    "    mod_int8_8 = relay.quantize.quantize(mod8, params8)\n",
    "mod_int8_8 = relay.transform.InferType()(mod_int8_8)\n",
    "\n",
    "# 3 · Build\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib8 = relay.build(mod_int8_8, target=target, params=params8)\n",
    "\n",
    "module8 = tvm.contrib.graph_executor.GraphModule(lib8['default'](dev))\n",
    "\n",
    "# 4 · Benchmark\n",
    "batch8 = np.random.rand(8,1,28,28).astype('float32')\n",
    "module8.set_input('input', batch8)\n",
    "for _ in range(5): module8.run()        # warm-up\n",
    "t8 = module8.module.time_evaluator(\"run\", dev, repeat=100)\n",
    "tvm_ms8 = np.median(t8().results)*1000\n",
    "print(f\"TVM INT8 batch-8 : {tvm_ms8:.3f} ms  → {tvm_ms8/8:.3f} ms / img\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c32b1089-3435-4e41-9632-506c0d6772ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch FP32 batch-8 : 0.755 ms  → 0.094 ms / img\n",
      "Throughput speed-up  : 3.29×\n"
     ]
    }
   ],
   "source": [
    "data8 = torch.randn(8,1,28,28)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(5): _ = model(data8)            # warm-up\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = model(data8)\n",
    "pt_ms8 = (time.time()-start)/100*1000\n",
    "print(f\"PyTorch FP32 batch-8 : {pt_ms8:.3f} ms  → {pt_ms8/8:.3f} ms / img\")\n",
    "print(f\"Throughput speed-up  : {(pt_ms8/8)/(tvm_ms8/8):.2f}×\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5dd536-cf37-4886-a3ff-f0b532f0f634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlhw-env)",
   "language": "python",
   "name": "mlhw-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
